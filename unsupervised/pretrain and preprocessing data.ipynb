{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dfd2862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow.compat.v1 as tfv1\n",
    "import tensorflow as tfv2\n",
    "tfv1.disable_eager_execution()\n",
    "import sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdeca317",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6bb3e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## take all data\n",
    "import pandas as pd\n",
    "col_names = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "\n",
    "data1 = pd.read_csv('./training.1600000.processed.noemoticon.csv',\n",
    "            encoding = \"ISO-8859-1\",\n",
    "            names=col_names)\n",
    "data = pd.DataFrame(columns= [\"sentiment\",\"text\"])\n",
    "data.sentiment = data1.target\n",
    "data.text = data1.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fe9ead4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take part of data\n",
    "col_names = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "\n",
    "data1 = pd.read_csv('./training.1600000.processed.noemoticon.csv',\n",
    "            encoding = \"ISO-8859-1\",\n",
    "            names=col_names)\n",
    "\n",
    "data_0 = data1[data1['target'] == 0].loc[:7800]\n",
    "data_1 = data1[data1['target'] == 4].loc[800000:807800]\n",
    "data_3 = pd.concat([data_0,data_1])\n",
    "\n",
    "\n",
    "data = pd.DataFrame(columns= [\"sentiment\",\"text\"])\n",
    "data.sentiment = data_3.target\n",
    "data.text = data_3.text\n",
    "data = sklearn.utils.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "948fff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove @XXXXX #XXX number and links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c40b2181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove(pattern,text):\n",
    "    document = []\n",
    "    for doc in text:\n",
    "        tmp = re.sub(pattern, \"\", doc)\n",
    "        document.append(tmp)\n",
    "    return document\n",
    "\n",
    "text = \"-Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\"\n",
    "\n",
    "\n",
    "\n",
    "pattern1 = r'http[a-z]*:+\\S+'\n",
    "pattern2 = r'@[A-Za-z0-9_.]*'\n",
    "pattern3 = r'#[A-Za-z0-9_.]*'\n",
    "pattern4 = r'[0-9]'\n",
    "\n",
    "#document = re.sub(pattern1, \"\", text)\n",
    "#document = re.sub(pattern2, \"\", document)\n",
    "#document = re.sub(pattern3, \"\", document)\n",
    "#document = re.sub(pattern4, \"\", document)\n",
    "\n",
    "\n",
    "document = remove(pattern1, data.text)\n",
    "document = remove(pattern2, document)\n",
    "document = remove(pattern3, document)\n",
    "document = remove(pattern4, document)\n",
    "\n",
    "\n",
    "#view = \"#\"\n",
    "#check = []\n",
    "#for text in document:\n",
    "#    check.append(view in text)\n",
    "\n",
    "##get label\n",
    "label = []\n",
    "for doc in data.sentiment:\n",
    "    label.append(doc)\n",
    "## get color list for PCA plot\n",
    "color = []\n",
    "for tmp in label:\n",
    "    if tmp == \"neutral\":\n",
    "        color.append(\"Blue\")\n",
    "    elif tmp == \"positive\":\n",
    "        color.append(\"Red\")\n",
    "    elif tmp == \"negative\":\n",
    "        color.append(\"Green\")\n",
    "\n",
    "## tokenize\n",
    "import tokenizers.pre_tokenizers\n",
    "tokenizers = tokenizers.pre_tokenizers.Sequence([tokenizers.pre_tokenizers.Punctuation(),tokenizers.pre_tokenizers.Whitespace()])\n",
    "document_tokenize = [] \n",
    "for sentence in document:\n",
    "    tokens = tokenizers.pre_tokenize_str(sentence)\n",
    "    tmp = []\n",
    "    for word in tokens:\n",
    "        tmp.append(word[0])\n",
    "    document_tokenize.append(tmp)\n",
    "\n",
    "## remove all empty\n",
    "check = []\n",
    "for i in range(0,len(document_tokenize)):\n",
    "    if len(document_tokenize[i]) == 0 :\n",
    "        check.append(i)\n",
    "\n",
    "for i in range(0,len(check)):\n",
    "    index = check[i] - i\n",
    "    del (document[index])\n",
    "    del (document_tokenize[index])\n",
    "    del (label[index])    \n",
    "    \n",
    "## save token_length   \n",
    "token_length = [len(sen) for sen in document_tokenize]\n",
    "import pickle\n",
    "test = open(\"token_length.pickle\",\"wb\")\n",
    "pickle.dump(token_length, test)\n",
    "test.close()    \n",
    "\n",
    "max_num  = max(len(sen) for sen in document_tokenize)\n",
    "document_tokenize_padding = []\n",
    "for sequence in document_tokenize:\n",
    "    tmp = sequence\n",
    "    for i in range(0,max_num-len(tmp)):\n",
    "        tmp.append(\"\")\n",
    "    document_tokenize_padding.append(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41b83729",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load elmo\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/3\",trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "030969c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8f39a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff28bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sequence_input = document_tokenize_padding\n",
    "token_length = [len(sen) for sen in document_tokenize]\n",
    "inputs = {\"tokens\": sequence_input,\"sequence_len\":token_length}\n",
    "\n",
    "def elmo_to_vectors(sequence_input,layer,types):\n",
    "    embeddings = elmo( inputs = sequence_input, signature=\"tokens\", as_dict=True)[layer]\n",
    "    init1 = tfv1.global_variables_initializer()\n",
    "    init2 = tfv1.tables_initializer()\n",
    "    sess = tfv1.Session()\n",
    "    sess.run(init1)\n",
    "    sess.run(init2)\n",
    "    \n",
    "    if types == \"maxout\":\n",
    "        maxout = tfv2.math.reduce_max(embeddings,axis=1)\n",
    "        maxout = sess.run(maxout)\n",
    "        return maxout\n",
    "    elif types == \"meanout\":\n",
    "        meanout = tfv2.math.reduce_mean(embeddings,axis=1)\n",
    "        meanout = sess.run(meanout)\n",
    "        return meanout\n",
    "    else:\n",
    "        embeddings_vectors = sess.run(embeddings)\n",
    "        return embeddings_vectors\n",
    "          \n",
    "\n",
    "    return embeddings_vectors,euclidean_norm,logsumexp,maxout,meanout\n",
    "def iteration_to_vect(inputs,layer,number,types):\n",
    "    results = []\n",
    "\n",
    "    for i in range(0,len(inputs['sequence_len']),number):\n",
    "        each = len(inputs['sequence_len']) // 100+1\n",
    "        percent = i // each \n",
    "        print(\"[\",\"-\"*percent,\" \"*(100-percent),\"]\",percent,\"%\",\"(\",i,\")\")\n",
    "        inputs_tmp = {\"tokens\":inputs['tokens'][i:i+number],\"sequence_len\":inputs['sequence_len'][i:i+number]}\n",
    "        results_tmp = elmo_to_vectors(inputs_tmp,layer,types)\n",
    "        results = results + results_tmp.tolist()\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8511161",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data1\n",
    "del document_tokenize\n",
    "del document\n",
    "import gc \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ab5523",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get elmo layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19dcba0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "layer = \"elmo\"\n",
    "types = \"embeddings_vectors\"\n",
    "embeddings_vectors_elmo = iteration_to_vect(inputs,layer,100,types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2171c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "types = \"maxout\"\n",
    "maxout_elmo  = iteration_to_vect(inputs,layer,100,types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee67f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "types = \"meanout\"\n",
    "meanout_emo = iteration_to_vect(inputs,layer,100,types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0004e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "test = open(\"label.pickle\",\"wb\")\n",
    "pickle.dump(label, test)\n",
    "test.close()\n",
    "\n",
    "test = open(\"embeddings_vectors_elmo.pickle\",\"wb\")\n",
    "pickle.dump(embeddings_vectors_elmo, test)\n",
    "test.close()\n",
    "\n",
    "test = open(\"maxout_elmo.pickle\",\"wb\")\n",
    "pickle.dump(maxout_elmo, test)\n",
    "test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c05f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = open(\"meanout_emo.pickle\",\"wb\")\n",
    "pickle.dump(meanout_emo, test)\n",
    "test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374df26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = \"word_emb\"\n",
    "types = \"embeddings_vectors\"\n",
    "embeddings_vectors_word_emb = iteration_to_vect(inputs,layer,100,types)\n",
    "types = \"maxout\"\n",
    "maxout_word_emb = iteration_to_vect(inputs,layer,100,types)\n",
    "types = \"meanout\"\n",
    "meanout_word_emb = iteration_to_vect(inputs,layer,100,types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ade143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = open(\"embeddings_vectors_word_emb.pickle\",\"wb\")\n",
    "pickle.dump(embeddings_vectors_word_emb, test)\n",
    "test.close()\n",
    "\n",
    "test = open(\"maxout_word_emb.pickle\",\"wb\")\n",
    "pickle.dump(maxout_word_emb, test)\n",
    "test.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2ce262",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = open(\"meanout_word_emb.pickle\",\"wb\")\n",
    "pickle.dump(meanout_word_emb, test)\n",
    "test.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1828437",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = \"lstm_outputs1\"   \n",
    "types = \"embeddings_vectors\"\n",
    "embeddings_vectors_lstm_outputs1 = iteration_to_vect(inputs,layer,100,types)\n",
    "types = \"maxout\"\n",
    "maxout_lstm_outputs1 = iteration_to_vect(inputs,layer,100,types)\n",
    "types = \"meanout\"\n",
    "meanout_lstm_outputs1 = iteration_to_vect(inputs,layer,100,types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45b6e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = open(\"embeddings_vectors_lstm_outputs1.pickle\",\"wb\")\n",
    "pickle.dump(embeddings_vectors_lstm_outputs1, test)\n",
    "test.close()\n",
    "\n",
    "test = open(\"maxout_lstm_outputs1.pickle\",\"wb\")\n",
    "pickle.dump(maxout_word_emb, test)\n",
    "test.close()\n",
    "\n",
    "test = open(\"meanout_lstm_outputs1.pickle\",\"wb\")\n",
    "pickle.dump(meanout_lstm_outputs1, test)\n",
    "test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15d029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = \"lstm_outputs2\"   \n",
    "types = \"embeddings_vectors\"\n",
    "embeddings_vectors_lstm_outputs2 = iteration_to_vect(inputs,layer,100,types)\n",
    "types = \"maxout\"\n",
    "maxout_lstm_outputs2 = iteration_to_vect(inputs,layer,100,types)\n",
    "types = \"meanout\"\n",
    "meanout_lstm_outputs2 = iteration_to_vect(inputs,layer,100,types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5880d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = open(\"lstm_outputs2.pickle\",\"wb\")\n",
    "pickle.dump(lstm_outputs2, test)\n",
    "test.close()\n",
    "\n",
    "test = open(\"lstm_outputs2.pickle\",\"wb\")\n",
    "pickle.dump(lstm_outputs2, test)\n",
    "test.close()\n",
    "\n",
    "test = open(\"lstm_outputs2.pickle\",\"wb\")\n",
    "pickle.dump(lstm_outputs2, test)\n",
    "test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c150b3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8788d76b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
