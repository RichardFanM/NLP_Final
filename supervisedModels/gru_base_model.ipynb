{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>156941</th>\n",
       "      <td>0</td>\n",
       "      <td>I yawn at  Palm Pre too. iPhone users unite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115375</th>\n",
       "      <td>0</td>\n",
       "      <td>@texanoutofwater so true!!! @wendywings is the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14556</th>\n",
       "      <td>1</td>\n",
       "      <td>@mileycyrus  what happened? idk why, but i sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236811</th>\n",
       "      <td>1</td>\n",
       "      <td>I miss my car, Commander Dickthrust  I hope my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187629</th>\n",
       "      <td>1</td>\n",
       "      <td>@Gertuzz: Nope.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230495</th>\n",
       "      <td>1</td>\n",
       "      <td>didn't make it through work. Docs app later to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197082</th>\n",
       "      <td>0</td>\n",
       "      <td>@CabreraNina i havee kwentoo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215192</th>\n",
       "      <td>1</td>\n",
       "      <td>watching tv via livestream; my tvcard has a pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15926</th>\n",
       "      <td>1</td>\n",
       "      <td>@meltinghalo Awesome! I am work today, double ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79415</th>\n",
       "      <td>0</td>\n",
       "      <td>@Cynyma Just dropping by for a twitter hello! ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment                                              tweet\n",
       "156941          0       I yawn at  Palm Pre too. iPhone users unite \n",
       "115375          0  @texanoutofwater so true!!! @wendywings is the...\n",
       "14556           1  @mileycyrus  what happened? idk why, but i sen...\n",
       "236811          1  I miss my car, Commander Dickthrust  I hope my...\n",
       "187629          1                                   @Gertuzz: Nope. \n",
       "230495          1  didn't make it through work. Docs app later to...\n",
       "197082          0                     @CabreraNina i havee kwentoo! \n",
       "215192          1  watching tv via livestream; my tvcard has a pr...\n",
       "15926           1  @meltinghalo Awesome! I am work today, double ...\n",
       "79415           0  @Cynyma Just dropping by for a twitter hello! ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Prepare\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "import re\n",
    "\n",
    "# df = pd.read_csv('./airline_sentiment_2_w_aa.csv')\n",
    "\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "df = pd.read_csv('./training.1600000.processed.noemoticon.csv',encoding=DATASET_ENCODING)\n",
    "df= df.iloc[:,[0,-1]]\n",
    "df.columns = ['sentiment','tweet']\n",
    "df = pd.concat([df.query(\"sentiment==0\").sample(120000),df.query(\"sentiment==4\").sample(120000)])\n",
    "df.sentiment = df.sentiment.map({0:1,4:0})\n",
    "df =  shuffle(df).reset_index(drop=True)\n",
    "\n",
    "# columns = df.columns\n",
    "# index_X = columns.get_loc('text')\n",
    "# index_Y = columns.get_loc('airline_sentiment')\n",
    "# df= df.iloc[:,[index_Y,index_X]]\n",
    "# df.columns = ['sentiment','tweet']\n",
    "# df.sentiment = df.sentiment.map({\"negative\":-1,\"positive\":1,\"neutral\":0})\n",
    "# df =  shuffle(df).reset_index(drop=True)\n",
    "\n",
    "df.sample(10)\n",
    "\n",
    "\n",
    "# X = []\n",
    "# Y = []\n",
    "# for row in data.values:\n",
    "#     X.append(row[index_X])\n",
    "#     Y.append(row[index_Y])\n",
    "\n",
    "# data = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "# for train, test in data.split(X):\n",
    "#     x_train, x_test = X[train], X[test]\n",
    "    # y_train, y_test = Y[train], Y[test]\n",
    "\n",
    "# print(len(x_test))\n",
    "\n",
    "# print(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.vocab import GloVe\n",
    "import torch\n",
    "from torchtext import data\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import pickle\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    120000\n",
       "0    120000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df):\n",
    "    '''Function to preprocess and create corpus'''\n",
    "    new_corpus=[]\n",
    "    vocab={}\n",
    "\n",
    "    for text in tqdm(df[\"tweet\"]):\n",
    "        tweet = text.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "        words=[w.translate(tweet) for w in re.sub(r'^@[a-zA-Z]+(_[a-zA-Z]+)?','',text).split() ]\n",
    "        for word in words:\n",
    "          try:  \n",
    "            vocab[word]+=1\n",
    "          except:\n",
    "            vocab[word]=1\n",
    "\n",
    "        new_corpus.append(words)\n",
    "    return new_corpus,vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240000/240000 [00:03<00:00, 68882.11it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus,vocab = preprocess_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)\n",
    "\n",
    "SEED = 2020\n",
    "EMBED_SIZE = 200\n",
    "MAX_FE = 1000\n",
    "MAX_LEN = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(corpus,mode='train'):\n",
    "\n",
    "  model_save_name = 'tokenizer.pickle'\n",
    "  path = F\"{model_save_name}\" \n",
    "     \n",
    "\n",
    "  if mode==\"train\":\n",
    "    tokenizer_obj=Tokenizer()\n",
    "    tokenizer_obj.fit_on_texts(corpus)\n",
    "    word_index=tokenizer_obj.word_index\n",
    "\n",
    "    with open(path,'wb') as tok:\n",
    "      pickle.dump(tokenizer_obj,tok,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "  else:\n",
    " \n",
    "    word_index=None\n",
    "    with open('tokenizer.pickle','rb') as tok:\n",
    "       tokenizer= pickle.load(tok)\n",
    "\n",
    "  \n",
    "  sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
    "  tweet_pad=pad_sequences(sequences,\n",
    "                            maxlen=MAX_LEN,\n",
    "                            truncating='post',\n",
    "                            padding='post')\n",
    "    \n",
    "  return tweet_pad,word_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_pad,word_index = tokenizer(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def prepare_matrix(word_index):\n",
    "    embedding_dict = GloVe(\"twitter.27B\",dim=200)\n",
    "    iiv= defaultdict(int)\n",
    "    oov= defaultdict(int)\n",
    "    num_words = len(word_index)\n",
    "    embedding_matrix = np.zeros((num_words+1, 200))\n",
    "\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i > num_words:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        emb_vec = embedding_dict[word]\n",
    "        if not torch.equal(emb_vec,torch.zeros((200),dtype=torch.float)):\n",
    "          embedding_matrix[i] = emb_vec\n",
    "          iiv[word]=vocab[word]\n",
    "\n",
    "        elif torch.equal(embedding_dict[word.lower()],torch.zeros((200),dtype=torch.float)):\n",
    "          emb_vec = embedding_dict[word.lower()]\n",
    "          embedding_matrix[i] = emb_vec\n",
    "          iiv[word]=vocab[word]\n",
    "\n",
    "        elif torch.equal(embedding_dict[word.title()],torch.zeros((200),dtype=torch.float)):\n",
    "          emb_vec = embedding_dict[word.title()]\n",
    "          embedding_matrix[i] = emb_vec\n",
    "          iiv[word]=vocab[word]\n",
    "\n",
    "        else:\n",
    "          oov[word] = vocab[word]\n",
    "\n",
    "    return embedding_matrix,iiv,oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 232196/232196 [00:06<00:00, 38060.04it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = prepare_matrix(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(tweet_pad,df.sentiment.values,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_matrix, lstm_hidden_size=200, gru_hidden_size=128):\n",
    "\n",
    "        super(TweetModel,self).__init__()\n",
    "        self.embedding = nn.Embedding(*embedding_matrix.shape)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = True\n",
    "        self.embedding_dropout = nn.Dropout2d(0.1)\n",
    "\n",
    "        self.gru = nn.GRU(embedding_matrix.shape[1] , gru_hidden_size,num_layers=1, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "        self.Linear1 = nn.Linear(gru_hidden_size*5,16)\n",
    "        self.Linear2 = nn.Linear(16,1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h_embedding = self.embedding(x)\n",
    "\n",
    "        x, (x_h,x_c) = self.gru(h_embedding)\n",
    "\n",
    "        avg_pool = torch.mean(x, 1)\n",
    "        max_pool, _ = torch.max(x, 1)\n",
    "        concat = torch.cat((avg_pool,x_h,max_pool), 1)\n",
    "        concat = self.Linear1(concat)\n",
    "        out = torch.sigmoid(self.Linear2(concat))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "\n",
    "  def __init__(self,train,targets=None,mode=\"train\"):\n",
    "\n",
    "    self.train = train\n",
    "    self.mode=mode\n",
    "    self.targets = targets\n",
    "    \n",
    "  def __len__(self):\n",
    "\n",
    "    return len(self.train)\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "\n",
    "    x_train_fold =  torch.tensor(self.train[idx],dtype=torch.long)\n",
    "    if self.mode=='train':\n",
    "        y_train_fold = torch.tensor(self.targets[idx],dtype=torch.float32)\n",
    "        return x_train_fold, y_train_fold\n",
    "    else:\n",
    "        return x_train_fold,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train,target,embedding_matrix,nepochs=5,batch_size=64,test_split=0.05):\n",
    "     \n",
    "    X_train,X_test,y_train,y_test = train_test_split(train,target,test_size = test_split)\n",
    "\n",
    "    train_data = TweetDataset(X_train,y_train)\n",
    "    test_data = TweetDataset(X_test,y_test)\n",
    "\n",
    "    dataloaders= {\"train\":DataLoader(train_data,batch_size=batch_size,shuffle=True),\n",
    "                    \"valid\":DataLoader(test_data,batch_size=batch_size,shuffle=True)}\n",
    "\n",
    "    model = TweetModel(embedding_matrix)\n",
    "    \n",
    "    loss_fn = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=1e-3,weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, eps=1e-4, verbose=True)\n",
    "\n",
    "    best_loss = {'train':np.inf,\n",
    "                  \"valid\":np.inf}\n",
    "\n",
    "    for epoch in range(nepochs):\n",
    "\n",
    "        epoch_loss = {\"train\":0.00,\n",
    "                      \"valid\":0.00}\n",
    "\n",
    "        for phase in ['train','valid']:\n",
    "            if phase==\"train\":\n",
    "                model = model.train()\n",
    "            else:\n",
    "                model= model.eval()\n",
    "\n",
    "            running_loss = 0.00\n",
    "\n",
    "            for i,(x,y) in enumerate(dataloaders['train']):\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase==\"train\"):\n",
    "\n",
    "                    predict = model(x).squeeze()\n",
    "                    loss = loss_fn(predict,y)\n",
    "\n",
    "\n",
    "\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss+= loss.item()/len(dataloaders[phase])\n",
    "\n",
    "                epoch_loss[phase]=running_loss\n",
    "\n",
    "        \n",
    "        print(\"Epoch {}/{}   -   loss: {:5.5f}   -   val_loss: {:5.5f}\".format(epoch+1, nepochs, epoch_loss['train'], epoch_loss['valid']))\n",
    "        scheduler.step(epoch_loss['valid'])\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20   -   loss: 0.46675   -   val_loss: 6.93234\n",
      "Epoch 2/20   -   loss: 0.39272   -   val_loss: 5.72838\n",
      "Epoch 3/20   -   loss: 0.33101   -   val_loss: 4.46998\n",
      "Epoch 4/20   -   loss: 0.24805   -   val_loss: 2.89526\n",
      "Epoch 5/20   -   loss: 0.18611   -   val_loss: 2.29871\n",
      "Epoch 6/20   -   loss: 0.15536   -   val_loss: 1.72280\n",
      "Epoch 7/20   -   loss: 0.13360   -   val_loss: 1.44530\n",
      "Epoch 8/20   -   loss: 0.11547   -   val_loss: 1.34587\n",
      "Epoch 9/20   -   loss: 0.10590   -   val_loss: 1.08955\n",
      "Epoch 10/20   -   loss: 0.09650   -   val_loss: 1.12091\n",
      "Epoch 11/20   -   loss: 0.09228   -   val_loss: 0.97987\n",
      "Epoch 12/20   -   loss: 0.08579   -   val_loss: 0.84572\n",
      "Epoch 13/20   -   loss: 0.08263   -   val_loss: 0.87146\n",
      "Epoch 14/20   -   loss: 0.07895   -   val_loss: 0.86188\n",
      "Epoch 15/20   -   loss: 0.07610   -   val_loss: 0.74716\n",
      "Epoch 16/20   -   loss: 0.07401   -   val_loss: 0.78346\n",
      "Epoch 17/20   -   loss: 0.07242   -   val_loss: 0.73497\n",
      "Epoch 18/20   -   loss: 0.06866   -   val_loss: 0.76498\n",
      "Epoch 19/20   -   loss: 0.06571   -   val_loss: 0.71858\n",
      "Epoch 20/20   -   loss: 0.06895   -   val_loss: 0.68584\n"
     ]
    }
   ],
   "source": [
    "model = train_model(X_train,y_train,embedding_matrix[0],nepochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(TweetDataset(X_test,mode=\"test\"),batch_size=32,shuffle=False)\n",
    "model.eval()\n",
    "batch_preds = []\n",
    "predictions=[]\n",
    "for x,y in dataloader:\n",
    "    with torch.no_grad():\n",
    "        batch_preds = model(x)\n",
    "\n",
    "    predictions.append(batch_preds)\n",
    "\n",
    "predictions = torch.cat(predictions, dim=0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7694197856243459\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(np.round(predictions).squeeze(),y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-adversariallearning2022]",
   "language": "python",
   "name": "conda-env-.conda-adversariallearning2022-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
