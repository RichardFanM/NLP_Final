{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c83b506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow.compat.v1 as tfv1\n",
    "import tensorflow as tfv2\n",
    "tfv1.disable_eager_execution()\n",
    "import sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "612d9659",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f52e06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## take all data\n",
    "import pandas as pd\n",
    "col_names = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "\n",
    "data1 = pd.read_csv('./training.1600000.processed.noemoticon.csv',\n",
    "            encoding = \"ISO-8859-1\",\n",
    "            names=col_names)\n",
    "data = pd.DataFrame(columns= [\"sentiment\",\"text\"])\n",
    "data.sentiment = data1.target\n",
    "data.text = data1.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8c7e708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take part of data\n",
    "col_names = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "\n",
    "data1 = pd.read_csv('./training.1600000.processed.noemoticon.csv',\n",
    "            encoding = \"ISO-8859-1\",\n",
    "            names=col_names)\n",
    "\n",
    "data_0 = data1[data1['target'] == 0].loc[:7800]\n",
    "data_1 = data1[data1['target'] == 4].loc[800000:807800]\n",
    "data_3 = pd.concat([data_0,data_1])\n",
    "\n",
    "\n",
    "data = pd.DataFrame(columns= [\"sentiment\",\"text\"])\n",
    "data.sentiment = data_3.target\n",
    "data.text = data_3.text\n",
    "data = sklearn.utils.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e09f18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove @XXXXX #XXX number and links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edf57e36",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'token_length' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 75\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m     74\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_length.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m pickle\u001b[38;5;241m.\u001b[39mdump(\u001b[43mtoken_length\u001b[49m, test)\n\u001b[1;32m     76\u001b[0m test\u001b[38;5;241m.\u001b[39mclose()    \n\u001b[1;32m     78\u001b[0m max_num  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sen) \u001b[38;5;28;01mfor\u001b[39;00m sen \u001b[38;5;129;01min\u001b[39;00m document_tokenize)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'token_length' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def remove(pattern,text):\n",
    "    document = []\n",
    "    for doc in text:\n",
    "        tmp = re.sub(pattern, \"\", doc)\n",
    "        document.append(tmp)\n",
    "    return document\n",
    "\n",
    "text = \"-Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\"\n",
    "\n",
    "\n",
    "\n",
    "pattern1 = r'http[a-z]*:+\\S+'\n",
    "pattern2 = r'@[A-Za-z0-9_.]*'\n",
    "pattern3 = r'#[A-Za-z0-9_.]*'\n",
    "pattern4 = r'[0-9]'\n",
    "\n",
    "#document = re.sub(pattern1, \"\", text)\n",
    "#document = re.sub(pattern2, \"\", document)\n",
    "#document = re.sub(pattern3, \"\", document)\n",
    "#document = re.sub(pattern4, \"\", document)\n",
    "\n",
    "\n",
    "document = remove(pattern1, data.text)\n",
    "document = remove(pattern2, document)\n",
    "document = remove(pattern3, document)\n",
    "document = remove(pattern4, document)\n",
    "\n",
    "\n",
    "#view = \"#\"\n",
    "#check = []\n",
    "#for text in document:\n",
    "#    check.append(view in text)\n",
    "\n",
    "##get label\n",
    "label = []\n",
    "for doc in data.sentiment:\n",
    "    label.append(doc)\n",
    "## get color list for PCA plot\n",
    "color = []\n",
    "for tmp in label:\n",
    "    if tmp == \"neutral\":\n",
    "        color.append(\"Blue\")\n",
    "    elif tmp == \"positive\":\n",
    "        color.append(\"Red\")\n",
    "    elif tmp == \"negative\":\n",
    "        color.append(\"Green\")\n",
    "\n",
    "## tokenize\n",
    "import tokenizers.pre_tokenizers\n",
    "tokenizers = tokenizers.pre_tokenizers.Sequence([tokenizers.pre_tokenizers.Punctuation(),tokenizers.pre_tokenizers.Whitespace()])\n",
    "document_tokenize = [] \n",
    "for sentence in document:\n",
    "    tokens = tokenizers.pre_tokenize_str(sentence)\n",
    "    tmp = []\n",
    "    for word in tokens:\n",
    "        tmp.append(word[0])\n",
    "    document_tokenize.append(tmp)\n",
    "\n",
    "## remove all empty\n",
    "check = []\n",
    "for i in range(0,len(document_tokenize)):\n",
    "    if len(document_tokenize[i]) == 0 :\n",
    "        check.append(i)\n",
    "\n",
    "for i in range(0,len(check)):\n",
    "    index = check[i] - i\n",
    "    del (document[index])\n",
    "    del (document_tokenize[index])\n",
    "    del (label[index])    \n",
    "    \n",
    "## save token_length   \n",
    "import pickle\n",
    "test = open(\"token_length.pickle\",\"wb\")\n",
    "pickle.dump(token_length, test)\n",
    "test.close()    \n",
    "\n",
    "max_num  = max(len(sen) for sen in document_tokenize)\n",
    "document_tokenize_padding = []\n",
    "for sequence in document_tokenize:\n",
    "    tmp = sequence\n",
    "    for i in range(0,max_num-len(tmp)):\n",
    "        tmp.append(\"\")\n",
    "    document_tokenize_padding.append(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7248b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load elmo\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/3\",trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3691e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531a1cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999b2b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sequence_input = document_tokenize_padding\n",
    "token_length = [len(sen) for sen in document_tokenize]\n",
    "inputs = {\"tokens\": sequence_input,\"sequence_len\":token_length}\n",
    "\n",
    "def elmo_to_vectors(sequence_input,layer,types):\n",
    "    embeddings = elmo( inputs = sequence_input, signature=\"tokens\", as_dict=True)[layer]\n",
    "    init1 = tfv1.global_variables_initializer()\n",
    "    init2 = tfv1.tables_initializer()\n",
    "    sess = tfv1.Session()\n",
    "    sess.run(init1)\n",
    "    sess.run(init2)\n",
    "    \n",
    "    if types == \"maxout\":\n",
    "        maxout = tfv2.math.reduce_max(embeddings,axis=1)\n",
    "        maxout = sess.run(maxout)\n",
    "        return maxout\n",
    "    elif types == \"meanout\":\n",
    "        meanout = tfv2.math.reduce_mean(embeddings,axis=1)\n",
    "        meanout = sess.run(meanout)\n",
    "        return meanout\n",
    "    else:\n",
    "        embeddings_vectors = sess.run(embeddings)\n",
    "        return embeddings_vectors\n",
    "          \n",
    "\n",
    "    return embeddings_vectors,euclidean_norm,logsumexp,maxout,meanout\n",
    "def iteration_to_vect(inputs,layer,number,types):\n",
    "    results = []\n",
    "\n",
    "    for i in range(0,len(inputs['sequence_len']),number):\n",
    "        each = len(inputs['sequence_len']) // 100+1\n",
    "        percent = i // each \n",
    "        print(\"[\",\"-\"*percent,\" \"*(100-percent),\"]\",percent,\"%\",\"(\",i,\")\")\n",
    "        inputs_tmp = {\"tokens\":inputs['tokens'][i:i+number],\"sequence_len\":inputs['sequence_len'][i:i+number]}\n",
    "        results_tmp = elmo_to_vectors(inputs_tmp,layer,types)\n",
    "        results = results + results_tmp.tolist()\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27214d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data1\n",
    "del document_tokenize\n",
    "del document\n",
    "import gc \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93305d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get elmo layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83911b57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "layer = \"elmo\"\n",
    "types = \"embeddings_vectors\"\n",
    "embeddings_vectors_elmo = iteration_to_vect(inputs,layer,100,types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43903180",
   "metadata": {},
   "outputs": [],
   "source": [
    "types = \"maxout\"\n",
    "maxout_elmo  = iteration_to_vect(inputs,layer,100,types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f605c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "types = \"meanout\"\n",
    "meanout_emo = iteration_to_vect(inputs,layer,100,types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9321337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "test = open(\"label.pickle\",\"wb\")\n",
    "pickle.dump(label, test)\n",
    "test.close()\n",
    "\n",
    "test = open(\"embeddings_vectors_elmo.pickle\",\"wb\")\n",
    "pickle.dump(embeddings_vectors_elmo, test)\n",
    "test.close()\n",
    "\n",
    "test = open(\"maxout_elmo.pickle\",\"wb\")\n",
    "pickle.dump(maxout_elmo, test)\n",
    "test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a59c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = open(\"meanout_emo.pickle\",\"wb\")\n",
    "pickle.dump(meanout_emo, test)\n",
    "test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39d7bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = \"word_emb\"\n",
    "types = \"embeddings_vectors\"\n",
    "embeddings_vectors_word_emb = iteration_to_vect(inputs,layer,100,types)\n",
    "types = \"maxout\"\n",
    "maxout_word_emb = iteration_to_vect(inputs,layer,100,types)\n",
    "types = \"meanout\"\n",
    "meanout_word_emb = iteration_to_vect(inputs,layer,100,types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e660bb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = open(\"embeddings_vectors_word_emb.pickle\",\"wb\")\n",
    "pickle.dump(embeddings_vectors_word_emb, test)\n",
    "test.close()\n",
    "\n",
    "test = open(\"maxout_word_emb.pickle\",\"wb\")\n",
    "pickle.dump(maxout_word_emb, test)\n",
    "test.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0b7c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = open(\"meanout_word_emb.pickle\",\"wb\")\n",
    "pickle.dump(meanout_word_emb, test)\n",
    "test.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d815732",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = \"lstm_outputs1\"   \n",
    "types = \"embeddings_vectors\"\n",
    "embeddings_vectors_lstm_outputs1 = iteration_to_vect(inputs,layer,100,types)\n",
    "types = \"maxout\"\n",
    "maxout_lstm_outputs1 = iteration_to_vect(inputs,layer,100,types)\n",
    "types = \"meanout\"\n",
    "meanout_lstm_outputs1 = iteration_to_vect(inputs,layer,100,types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce7a09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = open(\"embeddings_vectors_lstm_outputs1.pickle\",\"wb\")\n",
    "pickle.dump(embeddings_vectors_lstm_outputs1, test)\n",
    "test.close()\n",
    "\n",
    "test = open(\"maxout_lstm_outputs1.pickle\",\"wb\")\n",
    "pickle.dump(maxout_word_emb, test)\n",
    "test.close()\n",
    "\n",
    "test = open(\"meanout_lstm_outputs1.pickle\",\"wb\")\n",
    "pickle.dump(meanout_lstm_outputs1, test)\n",
    "test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e93a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = \"lstm_outputs2\"   \n",
    "types = \"embeddings_vectors\"\n",
    "embeddings_vectors_lstm_outputs2 = iteration_to_vect(inputs,layer,100,types)\n",
    "types = \"maxout\"\n",
    "maxout_lstm_outputs2 = iteration_to_vect(inputs,layer,100,types)\n",
    "types = \"meanout\"\n",
    "meanout_lstm_outputs2 = iteration_to_vect(inputs,layer,100,types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc8b400",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = open(\"lstm_outputs2.pickle\",\"wb\")\n",
    "pickle.dump(lstm_outputs2, test)\n",
    "test.close()\n",
    "\n",
    "test = open(\"lstm_outputs2.pickle\",\"wb\")\n",
    "pickle.dump(lstm_outputs2, test)\n",
    "test.close()\n",
    "\n",
    "test = open(\"lstm_outputs2.pickle\",\"wb\")\n",
    "pickle.dump(lstm_outputs2, test)\n",
    "test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1735125b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a02cf03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:testpytorch]",
   "language": "python",
   "name": "conda-env-testpytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
